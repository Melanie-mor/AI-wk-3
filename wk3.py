# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FftbjW5FKSa-Dz5hkL0I1Y-qEVyYdWrF
"""

# Import necessary libraries
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score

# Load dataset
iris = load_iris()
X = pd.DataFrame(iris.data, columns=iris.feature_names)
y = pd.Series(iris.target)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train model
model = DecisionTreeClassifier()
model.fit(X_train, y_train)

# Predictions and evaluation
y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred, average='weighted'))
print("Recall:", recall_score(y_test, y_pred, average='weighted'))

import tensorflow as tf
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt

# Load MNIST dataset
(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()

# Normalize and reshape data
X_train = X_train.reshape(-1, 28, 28, 1).astype("float32") / 255
X_test = X_test.reshape(-1, 28, 28, 1).astype("float32") / 255

# Build CNN model
model = models.Sequential([
    layers.Conv2D(32, kernel_size=3, activation='relu', input_shape=(28,28,1)),
    layers.MaxPooling2D(pool_size=2),
    layers.Conv2D(64, kernel_size=3, activation='relu'),
    layers.MaxPooling2D(pool_size=2),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(10, activation='softmax')
])

# Compile model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train model
history = model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test))

# Evaluate model
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_acc:.4f}")

!pip install spacy
import spacy

# Load English NLP pipeline
nlp = spacy.load("en_core_web_sm")

text = "I love the new Samsung Galaxy phone! The camera quality is stunning. Beats my old iPhone by far."

# Run NLP pipeline
doc = nlp(text)

# Extract entities
for ent in doc.ents:
    print(f"{ent.text} — {ent.label_}")

positive_words = ["love", "great", "excellent", "amazing", "wonderful"]
negative_words = ["bad", "poor", "terrible", "disappointing"]

sentiment = "Neutral"
if any(word in text.lower() for word in positive_words):
    sentiment = "Positive"
elif any(word in text.lower() for word in negative_words):
    sentiment = "Negative"

print("Sentiment:", sentiment)

"""While spaCy efficiently extracts named entities, it may reflect cultural or linguistic biases embedded in its training data. The rule-based sentiment system—though useful—can misclassify nuanced phrases, which raises questions about fairness in automated text analysis."""

samples = [
    "I can't believe how good this phone is!",
    "This laptop is not bad, but not great either.",
    "Terrible battery life, but excellent screen.",
    "Absolutely loved it... until it stopped working."
]

for text in samples:
    sentiment = "Neutral"
    if any(word in text.lower() for word in positive_words):
        sentiment = "Positive"
    elif any(word in text.lower() for word in negative_words):
        sentiment = "Negative"

    print(f"{text} → Sentiment: {sentiment}")